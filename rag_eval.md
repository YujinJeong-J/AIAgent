# RAG 평가
이행2본부 AI이행담당 - KTSPACE

---

## RAG 평가

### 사용 도구 (Framework)

**AutoRAG**
- 주요 기능: RAG 파이프라인 생성 + 평가 기능 모두 포함
- 평가 방식: GPT 기반으로 평가 데이터를 생성하고, 이를 활용해 평가 메트릭 계산

**Ragas**
- 주요 기능: RAG 파이프라인 생성은 불가, 평가 메트릭만 제공
- 강점:
  - 다양한 평가 메트릭 제공
  - 그래프·온톨로지 기반으로 평가 데이터셋을 생성 가능 → 복합적인 평가에 유리

➡ 따라서 기본적으로 AutoRAG를 사용하되, 다양한 메트릭이 필요할 경우 Ragas를 보완적으로 차용할 계획.  
둘 다 오픈소스 기반으로 커스텀에 용이

**이슈 포인트**: 자원(Resource) 소모가 커질 수 있음  
**해결 방안**: AutoRAG를 기본 틀로 활용하되, 필요 부분만 Ragas 코드 차용

---

### 차별화 포인트
- AutoRAG는 RAG 파이프라인 생성 + 평가를 모두 지원해 빠른 구축과 자동화에 강점.
- Ragas를 통해 정답지가 없는 환경에서도 GPT 기반 평가가 가능하여 초기 세팅 부담을 줄임.
- Ragas는 Knowledge Graph 기반 멀티홉·추론형 질문 생성을 통해 고품질 테스트 데이터셋 확보 가능.
- 다양한 메트릭과 온톨로지 기반 평가로 복합적인 성능 측정이 가능.
- 두 도구를 조합하면 효율적 파이프라인 운영 + 정밀 평가 체계를 동시에 달성할 수 있음.

---

## 기본 AutoRAG Flow (8단계)

1. **Query — 질의**  
   - 평가 데이터셋 생성 및 질의  
   - 정규화(불필요한 기호 제거, 언어 감지 등)를 거쳐 성능 향상

2. **Query Expansion — 질의 확장**  
   - 짧거나 모호한 질문을 여러 버전으로 변환 → 검색 리콜 향상  
   - 방식:
     - 패러프레이즈 생성 (같은 의미, 다른 표현)
     - 복잡한 질문을 하위질문으로 분해
     - HyDE: 임시 답변을 생성 후, 해당 문단을 검색 표적으로 활용

3. **Retrieval — 자료 수집**  
   - BM25(키워드 기반) + VectorDB(임베딩 기반) 병행 → 하이브리드 검색  
   - BM25: 키워드 일치에 강점 (짧은 질문, 고유명사 유리)  
   - VectorDB: 표현이 달라도 의미가 비슷하면 매칭 가능  
   - 두 장점을 합산하여 점수 산정

4. **Passage Augmenter — 맥락 보강**  
   - 검색된 문단에 제목·날짜·출처 같은 메타데이터를 붙임  
   - 인접 문단을 함께 묶어 맥락을 살림

5. **Reranker — 정밀 재정렬**  
   - 질문과 문단을 하나의 텍스트처럼 연결해 크로스 인코더로 관련도 점수 산출  
   - 크로스 인코더: 질문과 문단을 하나의 텍스트처럼 연결해서 보고, 관련도 점수를 산출  
   - 상위 몇십 개 문단만 유지

6. **Filter — 중복/노이즈 제거**  
   - 점수 낮은 문단 제거  
   - 중복 문단 제거 (유사 문단 하나만 남김)  
   - 너무 짧거나 의미 없는 텍스트 제외

7. **Compressor — 압축/요약**  
   - 질문과 관련된 핵심 문장만 뽑거나 요약  
   - LLM 입력 토큰 절약

8. **Prompt Maker — 프롬프트 구성**  
   - 시스템 지시문: “근거에 기반해 답변하라, 출처 표기 필수”
   - 컨텍스트: 압축된 근거 블록
   - 사용자 질문: 원질문
   - → 세 요소를 합쳐 모델 입력 생성

9. **Generator — 답변 생성**  
   - LLM이 프롬프트를 기반으로 답변 작성  
   - 필요 시 출처 표시 포함

---

## 신한은행 적용 시 Flow 및 고려사항

1) **데이터 수집 및 전처리**  
   - 문서, log, db 등 원천 source를 모으고 텍스트 클리닝, 포맷 통일 수행

2) **평가 데이터 생성**  
   - 은행 도메인 특성상, 정답지가 명확하기 때문에, 따로 평가 데이터셋을 생성하지 않고 기존 정답성 데이터(실제 문서, 로그 등)를 활용  
   - 그렇지 않은 경우 Ragas를 통해 테스트 데이터 생성

3) **RAG 평가(검색평가)**  
   - 위 AUTO RAG의 평가 프로세스를 적용  
   - Retrieval 시 은행권은 BM25(키워드 기반 검색)가 성능이 좋을 것이라 생각됨 (개인적인 견해)  
   - 은행권의 문서(약관, 규정, 보고서, 업무 매뉴얼 등)는 형식화된 용어와 고유명사를 많이 포함  
   - 예: “주택담보대출 상환 유예 제도”, “금융감독원 지침 제2023-7호”  
   - 이런 고유 키워드가 정확히 일치해야 의미가 달라지기 때문에, 키워드 매칭 성능이 좋은 BM25가 유리하다 봄  
   - 반대로 임베딩(Vector) 기반 검색은 의미적 유사성은 잡아주지만, 법률·금융 문서처럼 단어 하나 차이가 중요한 경우 오히려 헷갈릴 수 있음.  
   - 따라서 벡터 검색보다 BM25 위주로 운영할 가능성 높음

   **평가 지표 생성**  
   - auto rag 메트릭 : precision, recall, nDCG, MRR  
   - Ragas 지표(정답지 없을 시) : Context Precision, Context Recall - 정답지가 없어도 가져온 문서가 얼마나 관련 있고 충분한지 자동 판정 

4) **응답 평가**  
   - 정답지 O : BLEU, ROUGE, BERTScore, SemScore, G-Eval(일관성, 유창성)  
   - 정답지 X : RAGAS Faithfulness(근거성), Response Relevancy(질문 적합성)

5) **메트릭 집계 및 분석**  
   - 검색평가 및 응답평가를 가중평균 하여 최종 RAG Score 계산

6) **평가 메트릭 기반 리포트 및 대시보드**  
   - 평가 메트릭 기반으로 리포트 보고서를 생성하여, 웹에 시각화  
   - 대시보드를 구성해 관리자가 RAG 관련 모니터링을 진행할 수 있도록 함

7) **운영 방안**  
   - MCP 도구로 사용할 것이기 때문에, 너무 무겁게는 말고 가볍게 만든 것을 지향  
   - 실시간 평가보다는 배치(batch) 평가 방식 고려

---


### 실행 주기 & 볼륨(비용 관리)

| 주기        | 세트                     |     샘플 수 | LLM Judge 적용 | 목적          |
| --------- | ---------------------- | -------: | -----------: | ----------- |
| **매일**    | Golden(소) + Rolling(소) | 200\~600 |  10\~30% 샘플만 | 연기/회귀 조기 경보 |
| **매주**    | Golden(풀) + Rolling(중) |   1k\~3k |      30\~60% | 추세·슬라이스 진단  |
| **릴리스 전** | Golden(풀) + 위험객체 집중    |   3k\~5k |     60\~100% | 품질 보증 게이트   |

> 비용 절감 팁: judge는 **샘플링/상한선**을 두고, 나머지는 라벨/휴리스틱 메트릭으로 커버.

---

### 예상 모니터링 화면
